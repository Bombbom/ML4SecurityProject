{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bombbom/ML4SecurityProject/blob/main/Vector_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector representation "
      ],
      "metadata": {
        "id": "zJJjWM_qaCes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BzcIb9DhQJBd",
        "outputId": "fac59f49-7c25-4816-d9f5-4702dc927d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "1VKgvnzHaHlx",
        "outputId": "92d76479-3863-4ffc-e89f-790fbde4d067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import pandas"
      ],
      "metadata": {
        "id": "LD88YNIIaTV2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_file(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
        "        fragment = []\n",
        "        fragment_val = 0\n",
        "        for line in file:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                continue\n",
        "            if \"-\" * 33 in line and fragment:\n",
        "                yield fragment, fragment_val\n",
        "                fragment = []\n",
        "            elif stripped.split()[0].isdigit():\n",
        "                if fragment:\n",
        "                    if stripped.isdigit():\n",
        "                        fragment_val = int(stripped)\n",
        "                    else:\n",
        "                        fragment.append(stripped)\n",
        "            else:\n",
        "                fragment.append(stripped)"
      ],
      "metadata": {
        "id": "AsmdxobraP4n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# keywords of solidity; immutable set\n",
        "keywords = frozenset(\n",
        "    {'bool', 'break', 'case', 'catch', 'const', 'continue', 'default', 'do', 'double', 'struct',\n",
        "     'else', 'enum', 'payable', 'function', 'modifier', 'emit', 'export', 'extern', 'false', 'constructor',\n",
        "     'float', 'if', 'contract', 'int', 'long', 'string', 'super', 'or', 'private', 'protected', 'noReentrancy',\n",
        "     'public', 'return', 'returns', 'assert', 'event', 'indexed', 'using', 'require', 'uint', 'onlyDaoChallenge',\n",
        "     'transfer', 'Transfer', 'Transaction', 'switch', 'pure', 'view', 'this', 'throw', 'true', 'try', 'revert',\n",
        "     'bytes', 'bytes4', 'bytes32', 'internal', 'external', 'union', 'constant', 'while', 'for', 'notExecuted',\n",
        "     'NULL', 'uint256', 'uint128', 'uint8', 'uint16', 'address', 'call', 'msg', 'value', 'sender', 'notConfirmed',\n",
        "     'private', 'onlyOwner', 'internal', 'onlyGovernor', 'onlyCommittee', 'onlyAdmin', 'onlyPlayers', 'ownerExists',\n",
        "     'onlyManager', 'onlyHuman', 'only_owner', 'onlyCongressMembers', 'preventReentry', 'noEther', 'onlyMembers',\n",
        "     'onlyProxyOwner', 'confirmed', 'mapping'})\n",
        "\n",
        "# holds known non-user-defined functions; immutable set\n",
        "main_set = frozenset({'function', 'constructor', 'modifier', 'contract'})\n",
        "\n",
        "# arguments in main function; immutable set\n",
        "main_args = frozenset({'argc', 'argv'})\n",
        "\n",
        "\n",
        "# input is a list of string lines\n",
        "def clean_fragment(fragment):\n",
        "    # dictionary; map function name to symbol name + number\n",
        "    fun_symbols = {}\n",
        "    # dictionary; map variable name to symbol name + number\n",
        "    var_symbols = {}\n",
        "\n",
        "    fun_count = 1\n",
        "    var_count = 1\n",
        "\n",
        "    # regular expression to catch multi-line comment\n",
        "    rx_comment = re.compile('\\*/\\s*$')\n",
        "    # regular expression to find function name candidates\n",
        "    rx_fun = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?=\\s*\\()')\n",
        "    # regular expression to find variable name candidates\n",
        "    # rx_var = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?!\\s*\\()')\n",
        "    rx_var = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?:(?=\\s*\\w+\\()|(?!\\s*\\w+))(?!\\s*\\()')\n",
        "\n",
        "    # final cleaned gadget output to return to interface\n",
        "    cleaned_fragment = []\n",
        "\n",
        "    for line in fragment:\n",
        "        # process if not the header line and not a multi-line commented line\n",
        "        if rx_comment.search(line) is None:\n",
        "            # remove all string literals (keep the quotes)\n",
        "            nostrlit_line = re.sub(r'\".*?\"', '\"\"', line)\n",
        "            # remove all character literals\n",
        "            nocharlit_line = re.sub(r\"'.*?'\", \"''\", nostrlit_line)\n",
        "            # replace any non-ASCII characters with empty string\n",
        "            ascii_line = re.sub(r'[^\\x00-\\x7f]', r'', nocharlit_line)\n",
        "\n",
        "            # return, in order, all regex matches at string list; preserves order for semantics\n",
        "            user_fun = rx_fun.findall(ascii_line)\n",
        "            user_var = rx_var.findall(ascii_line)\n",
        "\n",
        "            # Could easily make a \"clean fragment\" type class to prevent duplicate functionality\n",
        "            # of creating/comparing symbol names for functions and variables in much the same way.\n",
        "            # The comparison frozenset, symbol dictionaries, and counters would be class scope.\n",
        "            # So would only need to pass a string list and a string literal for symbol names to\n",
        "            # another function.\n",
        "            for fun_name in user_fun:\n",
        "                if len({fun_name}.difference(main_set)) != 0 and len({fun_name}.difference(keywords)) != 0:\n",
        "                    # DEBUG\n",
        "                    # print('comparing ' + str(fun_name + ' to ' + str(main_set)))\n",
        "                    # print(fun_name + ' diff len from main is ' + str(len({fun_name}.difference(main_set))))\n",
        "                    # print('comparing ' + str(fun_name + ' to ' + str(keywords)))\n",
        "                    # print(fun_name + ' diff len from keywords is ' + str(len({fun_name}.difference(keywords))))\n",
        "                    ###\n",
        "                    # check to see if function name already in dictionary\n",
        "                    if fun_name not in fun_symbols.keys():\n",
        "                        fun_symbols[fun_name] = 'FUN' + str(fun_count)\n",
        "                        fun_count += 1\n",
        "                    # ensure that only function name gets replaced (no variable name with same\n",
        "                    # identifier); uses positive lookforward\n",
        "                    ascii_line = re.sub(r'\\b(' + fun_name + r')\\b(?=\\s*\\()', fun_symbols[fun_name], ascii_line)\n",
        "\n",
        "            for var_name in user_var:\n",
        "                # next line is the nuanced difference between fun_name and var_name\n",
        "                if len({var_name}.difference(keywords)) != 0 and len({var_name}.difference(main_args)) != 0:\n",
        "                    # DEBUG\n",
        "                    # print('comparing ' + str(var_name + ' to ' + str(keywords)))\n",
        "                    # print(var_name + ' diff len from keywords is ' + str(len({var_name}.difference(keywords))))\n",
        "                    # print('comparing ' + str(var_name + ' to ' + str(main_args)))\n",
        "                    # print(var_name + ' diff len from main args is ' + str(len({var_name}.difference(main_args))))\n",
        "                    ###\n",
        "                    # check to see if variable name already in dictionary\n",
        "                    if var_name not in var_symbols.keys():\n",
        "                        var_symbols[var_name] = 'VAR' + str(var_count)\n",
        "                        var_count += 1\n",
        "                    # ensure that only variable name gets replaced (no function name with same\n",
        "                    # identifier; uses negative lookforward\n",
        "                    ascii_line = re.sub(r'\\b(' + var_name + r')\\b(?:(?=\\s*\\w+\\()|(?!\\s*\\w+))(?!\\s*\\()', \\\n",
        "                                        var_symbols[var_name], ascii_line)\n",
        "\n",
        "            cleaned_fragment.append(ascii_line)\n",
        "    # return the list of cleaned lines\n",
        "    return cleaned_fragment"
      ],
      "metadata": {
        "id": "1u_BzIiAaSXL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Sets for operators\n",
        "operators3 = {'<<=', '>>='}\n",
        "operators2 = {\n",
        "    '->', '++', '--',\n",
        "    '!~', '<<', '>>', '<=', '>=',\n",
        "    '==', '!=', '&&', '||', '+=',\n",
        "    '-=', '*=', '/=', '%=', '&=', '^=', '|='\n",
        "}\n",
        "operators1 = {\n",
        "    '(', ')', '[', ']', '.',\n",
        "    '+', '-', '*', '&', '/',\n",
        "    '%', '<', '>', '^', '|',\n",
        "    '=', ',', '?', ':', ';',\n",
        "    '{', '}'\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "Functionality to train Word2Vec model and vectorize fragments\n",
        "Trains Word2Vec model using list of tokenized fragments\n",
        "Uses trained model embeddings to create 2D fragment vectors\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FragmentVectorizer:\n",
        "    def __init__(self, vector_length):\n",
        "        self.fragments = []\n",
        "        self.vector_length = vector_length\n",
        "        self.forward_slices = 0\n",
        "        self.backward_slices = 0\n",
        "\n",
        "    \"\"\"\n",
        "    Takes a line of solidity code (string) as input\n",
        "    Tokenizes solidity code (breaks down into identifier, variables, keywords, operators)\n",
        "    Returns a list of tokens, preserving order in which they appear\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(line):\n",
        "        tmp, w = [], []\n",
        "        i = 0\n",
        "        while i < len(line):\n",
        "            # Ignore spaces and combine previously collected chars to form words\n",
        "            if line[i] == ' ':\n",
        "                tmp.append(''.join(w))\n",
        "                tmp.append(line[i])\n",
        "                w = []\n",
        "                i += 1\n",
        "            # Check operators and append to final list\n",
        "            elif line[i:i + 3] in operators3:\n",
        "                tmp.append(''.join(w))\n",
        "                tmp.append(line[i:i + 3])\n",
        "                w = []\n",
        "                i += 3\n",
        "            elif line[i:i + 2] in operators2:\n",
        "                tmp.append(''.join(w))\n",
        "                tmp.append(line[i:i + 2])\n",
        "                w = []\n",
        "                i += 2\n",
        "            elif line[i] in operators1:\n",
        "                tmp.append(''.join(w))\n",
        "                tmp.append(line[i])\n",
        "                w = []\n",
        "                i += 1\n",
        "            # Character appended to word list\n",
        "            else:\n",
        "                w.append(line[i])\n",
        "                i += 1\n",
        "        # Filter out irrelevant strings\n",
        "        res = list(filter(lambda c: c != '', tmp))\n",
        "        return list(filter(lambda c: c != ' ', res))\n",
        "\n",
        "    \"\"\"\n",
        "    Tokenize entire fragment\n",
        "    Tokenize each line and concatenate to one long list\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize_fragment(fragment):\n",
        "        tokenized = []\n",
        "        function_regex = re.compile('function(\\d)+')\n",
        "        backwards_slice = False\n",
        "        for line in fragment:\n",
        "            tokens = FragmentVectorizer.tokenize(line)\n",
        "            tokenized += tokens\n",
        "            if len(list(filter(function_regex.match, tokens))) > 0:\n",
        "                backwards_slice = True\n",
        "            else:\n",
        "                backwards_slice = False\n",
        "        return tokenized, backwards_slice\n",
        "\n",
        "    \"\"\"\n",
        "    Add input fragment to model\n",
        "    Tokenize fragment and buffer it to list\n",
        "    \"\"\"\n",
        "\n",
        "    def add_fragment(self, fragment):\n",
        "        tokenized_fragment, backwards_slice = FragmentVectorizer.tokenize_fragment(fragment)\n",
        "        self.fragments.append(tokenized_fragment)\n",
        "        if backwards_slice:\n",
        "            self.backward_slices += 1\n",
        "        else:\n",
        "            self.forward_slices += 1\n",
        "\n",
        "    \"\"\"\n",
        "    Uses Word2Vec to create a vector for each fragment\n",
        "    Gets a vector for the fragment by combining token embeddings\n",
        "    Number of tokens used is min of number_of_tokens and 100\n",
        "    \"\"\"\n",
        "\n",
        "    def vectorize(self, fragment):\n",
        "        tokenized_fragment, backwards_slice = FragmentVectorizer.tokenize_fragment(fragment)\n",
        "        vectors = np.zeros(shape=(100, self.vector_length))\n",
        "        if backwards_slice:\n",
        "            for i in range(min(len(tokenized_fragment), 100)):\n",
        "                vectors[100 - 1 - i] = self.embeddings[tokenized_fragment[len(tokenized_fragment) - 1 - i]]\n",
        "        else:\n",
        "            for i in range(min(len(tokenized_fragment), 100)):\n",
        "                vectors[i] = self.embeddings[tokenized_fragment[i]]\n",
        "        return vectors\n",
        "\n",
        "    \"\"\"\n",
        "    Done adding fragments, then train Word2Vec model\n",
        "    Only keep list of embeddings, delete model and list of fragments\n",
        "    \"\"\"\n",
        "\n",
        "    def train_model(self):\n",
        "        # Set min_count to 1 to prevent out-of-vocabulary errors\n",
        "        # model = Word2Vec(self.fragments, min_count=1, vector_size=self.vector_length, sg=0)\n",
        "        model = Word2Vec(self.fragments, min_count=1, size=self.vector_length, sg=0)  # sg=0: CBOW; sg=1: Skip-Gram\n",
        "        self.embeddings = model.wv\n",
        "        del model\n",
        "        del self.fragments"
      ],
      "metadata": {
        "id": "Oxjjjug0aalS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors_df(filename, vector_length=300):\n",
        "    fragments = []\n",
        "    count = 0\n",
        "    vectorizer = FragmentVectorizer(vector_length)\n",
        "    for fragment, val in parse_file(filename):\n",
        "        count += 1\n",
        "        print(\"Collecting fragments...\", count, end=\"\\r\")\n",
        "        vectorizer.add_fragment(fragment)\n",
        "        row = {\"fragment\": fragment, \"val\": val}\n",
        "        fragments.append(row)\n",
        "    print('Found {} forward slices and {} backward slices'\n",
        "          .format(vectorizer.forward_slices, vectorizer.backward_slices))\n",
        "    print()\n",
        "    print(\"Training model...\", end=\"\\r\")\n",
        "    vectorizer.train_model()\n",
        "    print()\n",
        "    vectors = []\n",
        "    count = 0\n",
        "    for fragment in fragments:\n",
        "        count += 1\n",
        "        print(\"Processing fragments...\", count, end=\"\\r\")\n",
        "        vector = vectorizer.vectorize(fragment[\"fragment\"])\n",
        "        row = {\"vector\": vector, \"val\": fragment[\"val\"]}\n",
        "        vectors.append(row)\n",
        "    print()\n",
        "    df = pandas.DataFrame(vectors)\n",
        "    return df"
      ],
      "metadata": {
        "id": "-QLwc5IQaf3R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/Submit/NT522_Project/dataset_30000_record.txt\"\n",
        "parse_file(filename)\n",
        "base = os.path.splitext(os.path.basename(filename))[0]\n",
        "vector_filename = base + \"_fragment_vectors.pkl\"\n",
        "vector_length = int(300)\n",
        "df = get_vectors_df(filename, vector_length)\n",
        "df.to_pickle(vector_filename)"
      ],
      "metadata": {
        "id": "jhUvA7CYahZE",
        "outputId": "d84d260c-fb0f-4c4c-8f54-29755772c42b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29999 forward slices and 0 backward slices\n",
            "\n",
            "\n",
            "Processing fragments... 29999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/dataset_30000_record_fragment_vectors.pkl /content/drive/MyDrive/Submit/NT522_Project/dataset_30000_record_fragment_vectors.pkl"
      ],
      "metadata": {
        "id": "YvaQLaoaVjSw"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}