{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bombbom/ML4SecurityProject/blob/main/BiLSTM-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiLSTM-attention"
      ],
      "metadata": {
        "id": "HrAABtlkV-kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HWzflSJfFZXT",
        "outputId": "66398d13-dcb1-4e69-a082-4672d17fb4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astor==0.7.1"
      ],
      "metadata": {
        "id": "DuYVjJjdZAKR",
        "outputId": "a4c19360-cfb0-462c-93e8-0ee812473431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting astor==0.7.1\n",
            "  Downloading astor-0.7.1-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: astor\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires astor~=0.8.1, but you have astor 0.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed astor-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# vector_filename =\"/home/bombbom/Desktop/ReChecker/reentrancy_1671_fragment_vectors.pkl\"\n",
        "vector_filename = \"/content/drive/MyDrive/Submit/NT522_Project/dataset_10000_record_fixed_fragment_vectors.pkl\"\n",
        "data = pd.read_pickle(vector_filename)\n",
        "# print(data)\n",
        "# df.to_csv(\"/home/bombbom/Desktop/ReChecker/reentrancy_1671_fragment_vectors.csv\")"
      ],
      "metadata": {
        "id": "uHQduWxsZApf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "# from keras.utils import to_categorical\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.utils import compute_class_weight\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, LSTM, ReLU, Activation\n",
        "# from keras.layers.recurrent import SimpleRNN\n",
        "# from keras.layers import SimpleRNN\n",
        "# from keras.optimizers import Adamax\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from parser import parameter_parser\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, ReLU, Activation, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from sklearn.utils import compute_class_weight\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.layers import Layer\n",
        "# from models.loss_draw import LossHistory\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "from sklearn.utils import class_weight"
      ],
      "metadata": {
        "id": "clyT9BazZGjF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "\n",
        "# save loss, acc\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = {'batch': [], 'epoch': []}\n",
        "        self.accuracy = {'batch': [], 'epoch': []}\n",
        "        self.val_loss = {'batch': [], 'epoch': []}\n",
        "        self.val_acc = {'batch': [], 'epoch': []}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses['batch'].append(logs.get('loss'))\n",
        "        self.accuracy['batch'].append(logs.get('acc'))\n",
        "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses['epoch'].append(logs.get('loss'))\n",
        "        self.accuracy['epoch'].append(logs.get('acc'))\n",
        "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def loss_plot(self, loss_type):\n",
        "        iters = range(len(self.losses[loss_type]))\n",
        "        plt.figure()\n",
        "        # acc\n",
        "        plt.plot(iters, self.accuracy[loss_type], lw=1.5, color='r', label='train acc', marker='.', markevery=2,\n",
        "                 mew=1.5)\n",
        "        # loss\n",
        "        plt.plot(iters, self.losses[loss_type], lw=1.5, color='g', label='train loss', marker='.', markevery=2, mew=1.5)\n",
        "        if loss_type == 'epoch':\n",
        "            # val_acc\n",
        "            plt.plot(iters, self.val_acc[loss_type], lw=1.5, color='b', label='val acc', marker='.', markevery=2,\n",
        "                     mew=1.5)\n",
        "            # val_loss\n",
        "            plt.plot(iters, self.val_loss[loss_type], lw=1.5, color='darkorange', label='val loss', marker='.',\n",
        "                     markevery=2, mew=1.5)\n",
        "        plt.grid(True)\n",
        "        plt.xlim(-0.1, 10)\n",
        "        plt.ylim(-0.01, 1.01)\n",
        "        plt.xlabel(loss_type)\n",
        "        plt.ylabel('ACC-LOSS')\n",
        "        plt.legend(loc=\"center right\")\n",
        "        plt.savefig(\"acc_loss.pdf\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "imopt5_BZH4A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = np.stack(data.iloc[:, 0].values)\n",
        "# vectors = vectors.reshape()\n",
        "labels = data.iloc[:, 1].values\n",
        "positive_idxs = np.where(labels == 1)[0]\n",
        "negative_idxs = np.where(labels == 0)[0]\n",
        "idxs = np.concatenate([positive_idxs, negative_idxs])\n",
        "# undersampled_negative_idxs = np.random.choice(negative_idxs, len(positive_idxs), replace=True)\n",
        "undersampled_negative_idxs = np.random.choice(negative_idxs, len(positive_idxs), replace=False)\n",
        "\n",
        "resampled_idxs = np.concatenate([positive_idxs, undersampled_negative_idxs])\n",
        "x_train, x_test, y_train, y_test = train_test_split(vectors[resampled_idxs], labels[resampled_idxs],train_size=0.8,random_state=1, stratify=labels[resampled_idxs])"
      ],
      "metadata": {
        "id": "fL6wmegYZKzM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "9j1tr8hEZMDr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n"
      ],
      "metadata": {
        "id": "XkI0uG79ZNXx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    follows these equations:\n",
        "    (1) u_t = tanh(W h_t + b)\n",
        "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
        "    (3) v_t = \\alpha_t * h_t, v in time t\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's.\n",
        "        # Should add a small epsilon as the workaround\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "\n",
        "        return weighted_input\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[1], input_shape[2]\n",
        "\n"
      ],
      "metadata": {
        "id": "T9419UP0ZQWO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Addition(Layer):\n",
        "    \"\"\"\n",
        "    This layer is supposed to add of all activation weight.\n",
        "    We split this from AttentionWithContext to help us getting the activation weights\n",
        "    follows this equation:\n",
        "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
        "\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Addition, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.output_dim = input_shape[-1]\n",
        "        super(Addition, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return K.sum(x, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)"
      ],
      "metadata": {
        "id": "cek-ZVzxZSEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, ReLU, Activation, Bidirectional\n",
        "from keras.layers import Layer"
      ],
      "metadata": {
        "id": "VauffHBAZTdo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights =  class_weight.compute_class_weight(class_weight='balanced', classes=[0,1], y=labels)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "LmFPWFCiZU_t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "# y_train = to_categorical(y_train)\n",
        "lr = 0.002\n",
        "batch_size = 100\n",
        "epochs = 30\n",
        "threshold = 0.5\n",
        "adamax = Adamax(lr)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(300, return_sequences=True), input_shape=(vectors.shape[1], vectors.shape[2])))\n",
        "# model.add(AttentionWithContext())\n",
        "model.add(AttentionWithContext())\n",
        "model.add(Addition())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(300))\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "# Lower learning rate to prevent divergence\n",
        "adamax = Adamax(lr)\n",
        "model.compile(adamax, 'categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "o29RUCruZWn9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"BiLSTM-Attention\"\n",
        "history = LossHistory()\n",
        "model.fit(x_train, y_train, batch_size=batch_size, class_weight=class_weights, epochs=epochs, verbose=1, callbacks=[history], validation_data=(x_test, y_test))\n",
        "# model.save_weights(name + \"_model.pkl\")\n",
        "history.loss_plot('epoch')"
      ],
      "metadata": {
        "id": "_yWo4Xc1ZZ5g",
        "outputId": "a8171fc8-3617-4fcc-d0a4-15a82be7ecfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "80/80 [==============================] - 236s 3s/step - loss: 0.5460 - accuracy: 0.7130 - val_loss: 0.4905 - val_accuracy: 0.7610\n",
            "Epoch 2/30\n",
            "80/80 [==============================] - 227s 3s/step - loss: 0.4534 - accuracy: 0.7811 - val_loss: 0.4265 - val_accuracy: 0.7950\n",
            "Epoch 3/30\n",
            "80/80 [==============================] - 214s 3s/step - loss: 0.4267 - accuracy: 0.7921 - val_loss: 0.4138 - val_accuracy: 0.7925\n",
            "Epoch 4/30\n",
            "80/80 [==============================] - 213s 3s/step - loss: 0.4084 - accuracy: 0.7985 - val_loss: 0.3940 - val_accuracy: 0.8055\n",
            "Epoch 5/30\n",
            "80/80 [==============================] - 212s 3s/step - loss: 0.3913 - accuracy: 0.8112 - val_loss: 0.3889 - val_accuracy: 0.8165\n",
            "Epoch 6/30\n",
            "80/80 [==============================] - 216s 3s/step - loss: 0.3794 - accuracy: 0.8123 - val_loss: 0.3731 - val_accuracy: 0.8230\n",
            "Epoch 7/30\n",
            "80/80 [==============================] - 219s 3s/step - loss: 0.3657 - accuracy: 0.8235 - val_loss: 0.3837 - val_accuracy: 0.8220\n",
            "Epoch 8/30\n",
            "80/80 [==============================] - 215s 3s/step - loss: 0.3578 - accuracy: 0.8270 - val_loss: 0.3779 - val_accuracy: 0.8250\n",
            "Epoch 9/30\n",
            "80/80 [==============================] - 223s 3s/step - loss: 0.3441 - accuracy: 0.8344 - val_loss: 0.3720 - val_accuracy: 0.8325\n",
            "Epoch 10/30\n",
            "80/80 [==============================] - 217s 3s/step - loss: 0.3367 - accuracy: 0.8379 - val_loss: 0.3641 - val_accuracy: 0.8345\n",
            "Epoch 11/30\n",
            "80/80 [==============================] - 209s 3s/step - loss: 0.3254 - accuracy: 0.8462 - val_loss: 0.3616 - val_accuracy: 0.8315\n",
            "Epoch 12/30\n",
            "80/80 [==============================] - 202s 3s/step - loss: 0.3168 - accuracy: 0.8524 - val_loss: 0.3732 - val_accuracy: 0.8305\n",
            "Epoch 13/30\n",
            "80/80 [==============================] - 204s 3s/step - loss: 0.3047 - accuracy: 0.8555 - val_loss: 0.3709 - val_accuracy: 0.8370\n",
            "Epoch 14/30\n",
            "80/80 [==============================] - 205s 3s/step - loss: 0.3017 - accuracy: 0.8593 - val_loss: 0.3648 - val_accuracy: 0.8335\n",
            "Epoch 15/30\n",
            "80/80 [==============================] - 199s 2s/step - loss: 0.2926 - accuracy: 0.8600 - val_loss: 0.3756 - val_accuracy: 0.8380\n",
            "Epoch 16/30\n",
            "80/80 [==============================] - 199s 2s/step - loss: 0.2905 - accuracy: 0.8616 - val_loss: 0.3822 - val_accuracy: 0.8410\n",
            "Epoch 17/30\n",
            "80/80 [==============================] - 197s 2s/step - loss: 0.2865 - accuracy: 0.8659 - val_loss: 0.3833 - val_accuracy: 0.8330\n",
            "Epoch 18/30\n",
            "80/80 [==============================] - 195s 2s/step - loss: 0.2781 - accuracy: 0.8680 - val_loss: 0.3883 - val_accuracy: 0.8475\n",
            "Epoch 19/30\n",
            "80/80 [==============================] - 199s 2s/step - loss: 0.2756 - accuracy: 0.8699 - val_loss: 0.3748 - val_accuracy: 0.8390\n",
            "Epoch 20/30\n",
            "80/80 [==============================] - 199s 2s/step - loss: 0.2704 - accuracy: 0.8704 - val_loss: 0.3853 - val_accuracy: 0.8400\n",
            "Epoch 21/30\n",
            "80/80 [==============================] - 198s 2s/step - loss: 0.2678 - accuracy: 0.8698 - val_loss: 0.3989 - val_accuracy: 0.8425\n",
            "Epoch 22/30\n",
            "80/80 [==============================] - 195s 2s/step - loss: 0.2616 - accuracy: 0.8755 - val_loss: 0.4258 - val_accuracy: 0.8420\n",
            "Epoch 23/30\n",
            "80/80 [==============================] - 193s 2s/step - loss: 0.2611 - accuracy: 0.8755 - val_loss: 0.4182 - val_accuracy: 0.8435\n",
            "Epoch 24/30\n",
            "80/80 [==============================] - 197s 2s/step - loss: 0.2544 - accuracy: 0.8774 - val_loss: 0.4058 - val_accuracy: 0.8450\n",
            "Epoch 25/30\n",
            "80/80 [==============================] - 194s 2s/step - loss: 0.2545 - accuracy: 0.8767 - val_loss: 0.4172 - val_accuracy: 0.8435\n",
            "Epoch 26/30\n",
            "80/80 [==============================] - 195s 2s/step - loss: 0.2527 - accuracy: 0.8815 - val_loss: 0.4088 - val_accuracy: 0.8580\n",
            "Epoch 27/30\n",
            "80/80 [==============================] - 202s 3s/step - loss: 0.2489 - accuracy: 0.8821 - val_loss: 0.4369 - val_accuracy: 0.8455\n",
            "Epoch 28/30\n",
            "80/80 [==============================] - 199s 2s/step - loss: 0.2482 - accuracy: 0.8840 - val_loss: 0.4138 - val_accuracy: 0.8510\n",
            "Epoch 29/30\n",
            "80/80 [==============================] - 195s 2s/step - loss: 0.2407 - accuracy: 0.8860 - val_loss: 0.4247 - val_accuracy: 0.8540\n",
            "Epoch 30/30\n",
            "80/80 [==============================] - 194s 2s/step - loss: 0.2376 - accuracy: 0.8864 - val_loss: 0.4363 - val_accuracy: 0.8470\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy_test = model.evaluate(x_test, y_test)\n",
        "_, accuracy_train = model.evaluate(x_train, y_train)\n",
        "print('Accuracy test : %.2f' % (accuracy_test*100))\n",
        "print('Accuracy train: %.2f' % (accuracy_train*100))\n",
        "# storeResults(\"BLSTM\",(accuracy_train*100),(accuracy_test*100))"
      ],
      "metadata": {
        "id": "k3zQYQMxZbXN",
        "outputId": "322b449f-7f70-4118-9ed9-527bd3e813e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 26s 396ms/step - loss: 0.4363 - accuracy: 0.8470\n",
            "250/250 [==============================] - 102s 408ms/step - loss: 0.2216 - accuracy: 0.8947\n",
            "Accuracy test : 84.70\n",
            "Accuracy train: 89.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print(\"Accuracy: \", values[1])\n",
        "predictions = (model.predict(x_test, batch_size=batch_size)).round()\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)).ravel()\n",
        "print('False positive rate(FP): ', fp / (fp + tn))\n",
        "print('False negative rate(FN): ', fn / (fn + tp))\n",
        "recall = tp / (tp + fn)\n",
        "print('Recall: ', recall)\n",
        "precision = tp / (tp + fp)\n",
        "print('Precision: ', precision)\n",
        "print('F1 score: ', (2 * precision * recall) / (precision + recall))"
      ],
      "metadata": {
        "id": "BbTdPFiPZcwx",
        "outputId": "99f77f0a-f60b-40b6-db9d-e6a9c1a18bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 19s 936ms/step - loss: 0.4363 - accuracy: 0.8470\n",
            "Accuracy:  0.847000002861023\n",
            "20/20 [==============================] - 17s 806ms/step\n",
            "False positive rate(FP):  0.128\n",
            "False negative rate(FN):  0.178\n",
            "Recall:  0.822\n",
            "Precision:  0.8652631578947368\n",
            "F1 score:  0.8430769230769231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/acc_loss.pdf /content/drive/MyDrive/Submit/NT522_Project/acc_loss.pdf"
      ],
      "metadata": {
        "id": "1s0sD-QAeWnD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tn, fp, fn, tp)"
      ],
      "metadata": {
        "id": "Q7vVuAJVepI1",
        "outputId": "5ec1f655-0bcb-46be-d513-756dcdf6cfad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "872 128 178 822\n"
          ]
        }
      ]
    }
  ]
}