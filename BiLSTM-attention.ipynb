{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bombbom/ML4SecurityProject/blob/main/BiLSTM-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiLSTM-attention"
      ],
      "metadata": {
        "id": "HrAABtlkV-kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astor==0.7.1"
      ],
      "metadata": {
        "id": "DuYVjJjdZAKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# vector_filename =\"/home/bombbom/Desktop/ReChecker/reentrancy_1671_fragment_vectors.pkl\"\n",
        "vector_filename = \"/content/drive/MyDrive/DACN/dataset_fragment_vectors.pkl\"\n",
        "data = pd.read_pickle(vector_filename)\n",
        "# print(data)\n",
        "# df.to_csv(\"/home/bombbom/Desktop/ReChecker/reentrancy_1671_fragment_vectors.csv\")"
      ],
      "metadata": {
        "id": "uHQduWxsZApf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "# from keras.utils import to_categorical\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.utils import compute_class_weight\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, LSTM, ReLU, Activation\n",
        "# from keras.layers.recurrent import SimpleRNN\n",
        "# from keras.layers import SimpleRNN\n",
        "# from keras.optimizers import Adamax\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from parser import parameter_parser\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, ReLU, Activation, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from sklearn.utils import compute_class_weight\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.layers import Layer\n",
        "# from models.loss_draw import LossHistory\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "from sklearn.utils import class_weight"
      ],
      "metadata": {
        "id": "clyT9BazZGjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "\n",
        "# save loss, acc\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = {'batch': [], 'epoch': []}\n",
        "        self.accuracy = {'batch': [], 'epoch': []}\n",
        "        self.val_loss = {'batch': [], 'epoch': []}\n",
        "        self.val_acc = {'batch': [], 'epoch': []}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses['batch'].append(logs.get('loss'))\n",
        "        self.accuracy['batch'].append(logs.get('acc'))\n",
        "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses['epoch'].append(logs.get('loss'))\n",
        "        self.accuracy['epoch'].append(logs.get('acc'))\n",
        "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def loss_plot(self, loss_type):\n",
        "        iters = range(len(self.losses[loss_type]))\n",
        "        plt.figure()\n",
        "        # acc\n",
        "        plt.plot(iters, self.accuracy[loss_type], lw=1.5, color='r', label='train acc', marker='.', markevery=2,\n",
        "                 mew=1.5)\n",
        "        # loss\n",
        "        plt.plot(iters, self.losses[loss_type], lw=1.5, color='g', label='train loss', marker='.', markevery=2, mew=1.5)\n",
        "        if loss_type == 'epoch':\n",
        "            # val_acc\n",
        "            plt.plot(iters, self.val_acc[loss_type], lw=1.5, color='b', label='val acc', marker='.', markevery=2,\n",
        "                     mew=1.5)\n",
        "            # val_loss\n",
        "            plt.plot(iters, self.val_loss[loss_type], lw=1.5, color='darkorange', label='val loss', marker='.',\n",
        "                     markevery=2, mew=1.5)\n",
        "        plt.grid(True)\n",
        "        plt.xlim(-0.1, 10)\n",
        "        plt.ylim(-0.01, 1.01)\n",
        "        plt.xlabel(loss_type)\n",
        "        plt.ylabel('ACC-LOSS')\n",
        "        plt.legend(loc=\"center right\")\n",
        "        plt.savefig(\"acc_loss.pdf\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "imopt5_BZH4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = np.stack(data.iloc[:, 0].values)\n",
        "# vectors = vectors.reshape()\n",
        "labels = data.iloc[:, 1].values\n",
        "positive_idxs = np.where(labels == 1)[0]\n",
        "negative_idxs = np.where(labels == 0)[0]\n",
        "idxs = np.concatenate([positive_idxs, negative_idxs])\n",
        "undersampled_negative_idxs = np.random.choice(negative_idxs, len(positive_idxs), replace=True)\n",
        "# undersampled_negative_idxs = np.random.choice(negative_idxs, len(positive_idxs), replace=False)\n",
        "\n",
        "resampled_idxs = np.concatenate([positive_idxs, undersampled_negative_idxs])\n",
        "x_train, x_test, y_train, y_test = train_test_split(vectors[resampled_idxs], labels[resampled_idxs],train_size=0.8,random_state=1, stratify=labels[resampled_idxs])"
      ],
      "metadata": {
        "id": "fL6wmegYZKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "9j1tr8hEZMDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n"
      ],
      "metadata": {
        "id": "XkI0uG79ZNXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    follows these equations:\n",
        "    (1) u_t = tanh(W h_t + b)\n",
        "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
        "    (3) v_t = \\alpha_t * h_t, v in time t\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's.\n",
        "        # Should add a small epsilon as the workaround\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "\n",
        "        return weighted_input\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[1], input_shape[2]\n",
        "\n"
      ],
      "metadata": {
        "id": "T9419UP0ZQWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Addition(Layer):\n",
        "    \"\"\"\n",
        "    This layer is supposed to add of all activation weight.\n",
        "    We split this from AttentionWithContext to help us getting the activation weights\n",
        "    follows this equation:\n",
        "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
        "\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Addition, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.output_dim = input_shape[-1]\n",
        "        super(Addition, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return K.sum(x, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)"
      ],
      "metadata": {
        "id": "cek-ZVzxZSEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, ReLU, Activation, Bidirectional\n",
        "from keras.layers import Layer"
      ],
      "metadata": {
        "id": "VauffHBAZTdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights =  class_weight.compute_class_weight(class_weight='balanced', classes=[0,1], y=labels)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "LmFPWFCiZU_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "# y_train = to_categorical(y_train)\n",
        "lr = 0.002\n",
        "batch_size = 100\n",
        "epochs = 30\n",
        "threshold = 0.5\n",
        "adamax = Adamax(lr)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(300, return_sequences=True), input_shape=(vectors.shape[1], vectors.shape[2])))\n",
        "# model.add(AttentionWithContext())\n",
        "model.add(AttentionWithContext())\n",
        "model.add(Addition())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(300))\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "# Lower learning rate to prevent divergence\n",
        "adamax = Adamax(lr)\n",
        "model.compile(adamax, 'categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "o29RUCruZWn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"BiLSTM-Attention\"\n",
        "history = LossHistory()\n",
        "model.fit(x_train, y_train, batch_size=batch_size, class_weight=class_weights, epochs=epochs, verbose=1, callbacks=[history], validation_data=(x_test, y_test))\n",
        "# model.save_weights(name + \"_model.pkl\")\n",
        "history.loss_plot('epoch')"
      ],
      "metadata": {
        "id": "_yWo4Xc1ZZ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy_test = model.evaluate(x_test, y_test)\n",
        "_, accuracy_train = model.evaluate(x_train, y_train)\n",
        "print('Accuracy test : %.2f' % (accuracy_test*100))\n",
        "print('Accuracy train: %.2f' % (accuracy_train*100))\n",
        "# storeResults(\"BLSTM\",(accuracy_train*100),(accuracy_test*100))"
      ],
      "metadata": {
        "id": "k3zQYQMxZbXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print(\"Accuracy: \", values[1])\n",
        "predictions = (model.predict(x_test, batch_size=batch_size)).round()\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)).ravel()\n",
        "print('False positive rate(FP): ', fp / (fp + tn))\n",
        "print('False negative rate(FN): ', fn / (fn + tp))\n",
        "recall = tp / (tp + fn)\n",
        "print('Recall: ', recall)\n",
        "precision = tp / (tp + fp)\n",
        "print('Precision: ', precision)\n",
        "print('F1 score: ', (2 * precision * recall) / (precision + recall))"
      ],
      "metadata": {
        "id": "BbTdPFiPZcwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}